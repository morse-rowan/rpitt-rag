{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a9a6b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07895af",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "KNOWLEDGE_BASE_ID = os.getenv(\"BEDROCK_KNOWLEDGE_BASE_ID\")\n",
    "# Choose the model you want to use for generation\n",
    "MODEL_ID = os.getenv(\"BEDROCK_MODEL_ID_TITAN_G1_PREMIER\")\n",
    "AWS_REGION = os.getenv(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n",
    "AWS_PROFILE = os.getenv(\"AWS_PROFILE\") \n",
    "print(AWS_PROFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2223863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(profile_name=AWS_PROFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"creds = session.get_credentials().get_frozen_credentials()\n",
    "print(\"Access Key:\", creds.access_key)\n",
    "print(\"Secret Key:\", creds.secret_key)\n",
    "print(\"Session Token:\", creds.token)\n",
    "print(\"Profile Used:\", session.profile_name or \"default\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53ddd983",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_agent_runtime = session.client(\n",
    "    service_name=\"bedrock-agent-runtime\",\n",
    "    region_name=AWS_REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce35bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt_template(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Prompt file not found at {filepath}\")\n",
    "        return None # Or a default prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ba35537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-premier-v1:0\n"
     ]
    }
   ],
   "source": [
    "def _model_id_to_arn(model_id: str, region: str = AWS_REGION) -> str:\n",
    "    \"\"\"\n",
    "    Convert a Bedrock modelId to the modelArn string Bedrock APIs expect.\n",
    "    \"\"\"\n",
    "    return f\"arn:aws:bedrock:{region}::foundation-model/{model_id}\"\n",
    "\n",
    "MODEL_ARN = _model_id_to_arn(MODEL_ID,AWS_REGION)\n",
    "print(MODEL_ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40668b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_knowledge_base(query_text, prompt_template_text=None, number_of_results=5):\n",
    "    if not MODEL_ARN:\n",
    "        print(\"Error: Model ARN is not set. Please check your .env file and variable name (e.g., BEDROCK_MODEL_ARN_LLAMA3).\")\n",
    "        return None\n",
    "    if not KNOWLEDGE_BASE_ID:\n",
    "        print(\"Error: Knowledge Base ID is not set. Please check your .env file and variable name (BEDROCK_KNOWLEDGE_BASE_ID).\")\n",
    "        return None\n",
    "\n",
    "    # This is the payload structure that will be passed to retrieve_and_generate\n",
    "    # Note that knowledgeBaseId is NOT a top-level key in this payload,\n",
    "    # but modelArn is used inside knowledgeBaseConfiguration.\n",
    "    payload_for_api = {\n",
    "        \"input\": {\n",
    "            \"text\": query_text\n",
    "        },\n",
    "        \"retrieveAndGenerateConfiguration\": {\n",
    "            \"type\": \"KNOWLEDGE_BASE\",\n",
    "            \"knowledgeBaseConfiguration\": {\n",
    "                \"knowledgeBaseId\": KNOWLEDGE_BASE_ID,\n",
    "                # Use modelArn for generation within knowledgeBaseConfiguration\n",
    "                \"modelArn\": MODEL_ARN,\n",
    "                \"retrievalConfiguration\": {\n",
    "                    \"vectorSearchConfiguration\": {\n",
    "                        \"numberOfResults\": number_of_results\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        # sessionConfiguration can be added here if managing session state\n",
    "        # \"sessionConfiguration\": {\n",
    "        #     \"sessionId\": \"your-session-id\" # Optional\n",
    "        # }\n",
    "    }\n",
    "\n",
    "    if prompt_template_text:\n",
    "        kb_config = payload_for_api[\"retrieveAndGenerateConfiguration\"][\"knowledgeBaseConfiguration\"]\n",
    "        if \"generationConfiguration\" not in kb_config:\n",
    "            kb_config[\"generationConfiguration\"] = {}\n",
    "        \n",
    "        kb_config[\"generationConfiguration\"][\"promptTemplate\"] = {\n",
    "            \"textPromptTemplate\": prompt_template_text\n",
    "        }\n",
    "        # Add other model-specific inference parameters if needed:\n",
    "        # kb_config[\"generationConfiguration\"][\"inferenceConfig\"] = {\n",
    "        #     \"textGenerationConfig\": { # For Llama models\n",
    "        #         \"maxTokenCount\": 1024,\n",
    "        #         \"stopSequences\": [],\n",
    "        #         \"temperature\": 0.7,\n",
    "        #         \"topP\": 0.9\n",
    "        #     }\n",
    "        # }\n",
    "\n",
    "    #print(f\"Sending payload to Bedrock: {json.dumps(payload_for_api, indent=2)}\")\n",
    "\n",
    "    try:\n",
    "        # Call retrieve_and_generate by unpacking the payload_for_api dictionary.\n",
    "        # The knowledgeBaseId is part of the nested configuration, not a direct kwarg here.\n",
    "        response = bedrock_agent_runtime.retrieve_and_generate(\n",
    "            **payload_for_api\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Bedrock retrieve_and_generate: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89e385ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###  SYSTEM  ###############################################################\n",
      "You are **PittGPT**, the University of Pittsburgh’s question-answering assistant.\n",
      "Your answers MUST follow these rules:\n",
      "\n",
      "1. **Use ONLY the materials provided in <DOCUMENTS>.**  \n",
      "   • Do not invent new facts.  \n",
      "   • If the documents cannot answer, say so briefly and suggest where the user might look.\n",
      "\n",
      "2. **Cite every factual claim** with the Reddit author’s handle and a footnote number, e.g.  \n",
      "   *Office hours boosted my grade* — u/ListDisastrous549 [1]\n",
      "\n",
      "3. **List the source links at the very end** under the heading  \n",
      "   **“Original Reddit threads for verification”** using the same footnote numbers.\n",
      "\n",
      "4. **Freshness & reliability**  \n",
      "   • When a post is ≥ 3 years old OR the topic is likely to have changed (policies, tuition, etc.), add  \n",
      "   “*(Info from YYYY; may have changed.)*”    \n",
      "   • If sources conflict, say which view is stronger and why.\n",
      "\n",
      "5. **Tone** — helpful, Avoid jargon unless the user is technical.\n",
      "\n",
      "6. **Output format**\n",
      "    $output_format_instructions$\n",
      "\n",
      "    Original Reddit threads for verification\n",
      "    [1] <URL1>\n",
      "    [2] <URL2>\n",
      "    ... \n",
      "Do **NOT** output anything else (no JSON, no hidden tags).\n",
      "\n",
      "############################################################################\n",
      "\n",
      "<USER_QUESTION>\n",
      "$user_query$\n",
      "\n",
      "<DOCUMENTS>\n",
      "$search_results$\n",
      "### END OF PROMPT ##########################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_prompt_template(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Prompt file not found at {filepath}\")\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Error: Failed decoding {filepath} as UTF-8\")\n",
    "\n",
    "# then\n",
    "custom_prompt_filepath = '../prompts/prompt2.txt'\n",
    "generation_prompt = load_prompt_template(custom_prompt_filepath)\n",
    "print(generation_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f09d06fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_generated_response(bedrock_response: dict) -> None:\n",
    "    \"\"\"\n",
    "    Print only the model-generated answer.\n",
    "    \"\"\"\n",
    "    if not bedrock_response:\n",
    "        print(\"No response returned from Bedrock.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        text = bedrock_response[\"output\"][\"text\"]\n",
    "        print(\"\\n--- Generated Response ---\")\n",
    "        print(text)\n",
    "    except KeyError:\n",
    "        print(\"Expected key 'output.text' not found in Bedrock response.\")\n",
    "\n",
    "\n",
    "def print_citations(bedrock_response: dict) -> None:\n",
    "    \"\"\"\n",
    "    Print only the citation information (content snippet, S3 URI, metadata).\n",
    "    \"\"\"\n",
    "    if not bedrock_response:\n",
    "        print(\"No response returned from Bedrock.\")\n",
    "        return\n",
    "\n",
    "    citations = bedrock_response.get(\"citations\", [])\n",
    "    if not citations:\n",
    "        print(\"No citations found or citations format not as expected.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Retrieved Citations (Sources) ---\")\n",
    "    for i, citation_group in enumerate(citations, start=1):\n",
    "        print(f\"\\nCitation Group {i}:\")\n",
    "        refs = citation_group.get(\"retrievedReferences\", [])\n",
    "        if not refs:\n",
    "            print(\"  No retrieved references in this citation group.\")\n",
    "            continue\n",
    "\n",
    "        for j, ref in enumerate(refs, start=1):\n",
    "            print(f\"  Reference {j}:\")\n",
    "            snippet = (\n",
    "                ref.get(\"content\", {}).get(\"text\", \"\")\n",
    "            )\n",
    "            if snippet:\n",
    "                print(f\"    Content Snippet: {snippet[:300]}...\")\n",
    "            s3_uri = (\n",
    "                ref.get(\"location\", {})\n",
    "                   .get(\"s3Location\", {})\n",
    "                   .get(\"uri\")\n",
    "            )\n",
    "            if s3_uri:\n",
    "                print(f\"    S3 Location: {s3_uri}\")\n",
    "            metadata = ref.get(\"metadata\")\n",
    "            if metadata:\n",
    "                print(f\"    Metadata: {metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44043ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during Bedrock retrieve_and_generate: An error occurred (ExpiredTokenException) when calling the RetrieveAndGenerate operation: The security token included in the request is expired\n",
      "No response returned from Bedrock.\n",
      "No response returned from Bedrock.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if not MODEL_ARN:\n",
    "        print(\"Execution failed: MODEL_ARN not set. Check .env and environment variable name BEDROCK_MODEL_ARN_LLAMA3.\")\n",
    "    elif not KNOWLEDGE_BASE_ID:\n",
    "        print(\"Execution failed: KNOWLEDGE_BASE_ID not set. Check .env and environment variable name.\")\n",
    "    else:\n",
    "        user_query = (\n",
    "            \"I’m retaking Chem 1 over the summer and enrolled in her class because \"\n",
    "            \"I couldn’t enroll in the first summer session. I’m concerned by her 1.9 \"\n",
    "            \"on RateMyProfessor, especially because I did badly in the class the first time. \"\n",
    "            \"Has anyone taken her class, and is getting an A in her class doable?\"\n",
    "        )\n",
    "\n",
    "        bedrock_response = query_knowledge_base(\n",
    "            user_query,\n",
    "            prompt_template_text=generation_prompt\n",
    "        )\n",
    "\n",
    "        # Now call whichever view you need\n",
    "        print_generated_response(bedrock_response)\n",
    "        print_citations(bedrock_response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
